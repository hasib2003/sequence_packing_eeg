{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***jointly training 1 layer base 0.5 with only first segement training data***"
      ],
      "metadata": {
        "id": "OMiGu6IZtc_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU8L6qfmKAoE",
        "outputId": "96f2e95f-45fa-4a02-dfaa-082792cf7b94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mne"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkvkaPuAKSaK",
        "outputId": "6d5fd720-0374-492d-a8e0-94a4719cea24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mne in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mne) (3.1.4)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.10/dist-packages (from mne) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.10/dist-packages (from mne) (3.7.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from mne) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mne) (24.1)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.10/dist-packages (from mne) (1.8.2)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from mne) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mne) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->mne) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->mne) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->mne) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->mne) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->mne) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->mne) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne) (4.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mne) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->mne) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLvy5QFc61Df"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKj-z1i8Bq1N"
      },
      "outputs": [],
      "source": [
        "from dataset import Raw_PhysionNet,PSD_PhysioNet\n",
        "from torch.utils.data import  random_split,DataLoader\n",
        "import torch\n",
        "import os\n",
        "import config\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2n8bZOWJ99m"
      },
      "outputs": [],
      "source": [
        "num_channels = 64\n",
        "hidden_size = 128\n",
        "num_layers = 1\n",
        "NUM_SUBJS=106"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySNZhnfeCKCF"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uz9GEQJzpUXi"
      },
      "outputs": [],
      "source": [
        "base_dir = os.path.join(\"/content/drive/MyDrive\",\"EEG_AUTH_Experiments\",\"exp22\")\n",
        "if not os.path.isdir(base_dir):\n",
        "  os.makedirs(base_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sXbVsu1tvYl"
      },
      "outputs": [],
      "source": [
        "chk_point_best = os.path.join(base_dir,\"best_model.pth\")\n",
        "chk_point_last = os.path.join(base_dir,\"last_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ZTbOxaz3J99n",
        "outputId": "eb8ac287-6339-4def-c7c6-b619b1e231b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/EEG_AUTH_Experiments/exp22/best_model.pth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "chk_point_best"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"Modifies the batch by creating random sequence length on every input\"\"\"\n",
        "    # print(len(batch))\n",
        "    signals, labels = zip(*batch)\n",
        "\n",
        "    # Optional: Set seed to a random value to ensure randomness\n",
        "    torch.manual_seed(int(time.time() * 1000) % (2**32 - 1))\n",
        "\n",
        "    # Getting random length of signal to be used\n",
        "    sig_len = torch.randint(low=1, high=20, size=(1,)).item() / 10 # between (0.1 and 2.0)\n",
        "    # print(sig_len)\n",
        "\n",
        "    # Modify signals based on the generated random length\n",
        "    signals = tuple(sig[:,:int(sig_len * 160)] for sig in signals)\n",
        "\n",
        "    return signals, labels"
      ],
      "metadata": {
        "id": "zisLtIoJHvS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9QD5r84Bq1O"
      },
      "outputs": [],
      "source": [
        "def gen_dataloader(dataset,split_ratios):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        dataset: torch dataset\n",
        "        split_ratio: list of floats that sums to one, represeting the size of train ,val and test_set\n",
        "    \"\"\"\n",
        "\n",
        "    gen1= torch.Generator().manual_seed(37)\n",
        "\n",
        "    train,val = random_split(dataset,lengths=split_ratios,generator=gen1)\n",
        "    # train,val,test = random_split(dataset,lengths=split_ratios)\n",
        "\n",
        "    train_loader = DataLoader(train,batch_size=64,shuffle=True,collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val,batch_size=64,shuffle=False,collate_fn=collate_fn)\n",
        "    # test_loader = DataLoader(test,batch_size=32,shuffle=False)\n",
        "\n",
        "    return train_loader,val_loader\n",
        "    # return train_loader,val_loader,test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTdmd2WlBq1O"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, optimizer, criterion, num_epochs, loss_train, loss_val, acc_train, acc_val):\n",
        "    global best_eval_acc\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for batch_data, batch_labels in train_loader:\n",
        "            batch_data = batch_data.permute(0, 2, 1)\n",
        "            batch_data = batch_data.float().to(device)\n",
        "            batch_labels = batch_labels.long().to(device)\n",
        "\n",
        "            if batch_data.shape[0] == 1:\n",
        "                continue  # Skipping any batch size with only one example since batch normalization is being used\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_data)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            epoch_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_train += batch_labels.size(0)\n",
        "            correct_train += (predicted == batch_labels).sum().item()\n",
        "\n",
        "        # Calculate and save the epoch loss and accuracy for training\n",
        "        epoch_loss /= len(train_loader)\n",
        "        train_accuracy = correct_train / total_train\n",
        "        loss_train.append(epoch_loss)\n",
        "        acc_train.append(train_accuracy)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0.0\n",
        "            correct_val = 0\n",
        "            total_val = 0\n",
        "            for batch_data, batch_labels in val_loader:\n",
        "                batch_data = batch_data.permute(0, 2, 1)\n",
        "                batch_data = batch_data.float().to(device)\n",
        "                batch_labels = batch_labels.long().to(device)\n",
        "\n",
        "                outputs = model(batch_data)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total_val += batch_labels.size(0)\n",
        "                correct_val += (predicted == batch_labels).sum().item()\n",
        "                val_loss += criterion(outputs, batch_labels).item()\n",
        "\n",
        "            val_loss /= len(val_loader)\n",
        "            val_accuracy = correct_val / total_val\n",
        "\n",
        "            loss_val.append(val_loss)\n",
        "            acc_val.append(val_accuracy)\n",
        "\n",
        "        if val_accuracy > best_eval_acc:\n",
        "            print(f\"---- new best val acc achieved {val_accuracy} ----\")\n",
        "            torch.save(model.state_dict(), chk_point_best)\n",
        "            best_eval_acc = val_accuracy\n",
        "\n",
        "        torch.save(model.state_dict(), chk_point_last)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}')\n",
        "\n",
        "    return loss_train, loss_val, acc_train, acc_val\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_variable_length(model, train_loaders, val_loaders, optimizer, criterion, num_epochs, loss_train, loss_val, acc_train, acc_val):\n",
        "    global best_eval_acc\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_epoch_loss = 0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss_combined = 0\n",
        "        # Iterate over all training loaders\n",
        "        for train_loader in train_loaders:\n",
        "            for batch_data, batch_labels in train_loader:\n",
        "                batch_data = batch_data.permute(0, 2, 1)\n",
        "                batch_data = batch_data.float().to(device)\n",
        "                batch_labels = batch_labels.long().to(device)\n",
        "\n",
        "                if batch_data.shape[0] == 1:\n",
        "                    continue  # Skip batches with only one example\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(batch_data)\n",
        "                loss = criterion(outputs, batch_labels)\n",
        "                loss_combined = loss_combined + loss\n",
        "                total_epoch_loss += loss.item()\n",
        "\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total_train += batch_labels.size(0)\n",
        "                correct_train += (predicted == batch_labels).sum().item()\n",
        "\n",
        "\n",
        "\n",
        "        # Step after processing all loaders\n",
        "        loss_combined.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate and save the epoch loss and accuracy for training\n",
        "        total_epoch_loss /= sum(len(loader) for loader in train_loaders)\n",
        "        train_accuracy = correct_train / total_train\n",
        "        loss_train.append(total_epoch_loss)\n",
        "        acc_train.append(train_accuracy)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            total_val_loss = 0.0\n",
        "            correct_val = 0\n",
        "            total_val = 0\n",
        "\n",
        "            # Iterate over all validation loaders\n",
        "            for val_loader in val_loaders:\n",
        "                for batch_data, batch_labels in val_loader:\n",
        "                    batch_data = batch_data.permute(0, 2, 1)\n",
        "                    batch_data = batch_data.float().to(device)\n",
        "                    batch_labels = batch_labels.long().to(device)\n",
        "\n",
        "                    outputs = model(batch_data)\n",
        "                    loss = criterion(outputs, batch_labels)\n",
        "                    total_val_loss += loss.item()\n",
        "\n",
        "                    _, predicted = torch.max(outputs, 1)\n",
        "                    total_val += batch_labels.size(0)\n",
        "                    correct_val += (predicted == batch_labels).sum().item()\n",
        "\n",
        "            total_val_loss /= sum(len(loader) for loader in val_loaders)\n",
        "            val_accuracy = correct_val / total_val\n",
        "\n",
        "            loss_val.append(total_val_loss)\n",
        "            acc_val.append(val_accuracy)\n",
        "\n",
        "            if val_accuracy > best_eval_acc:\n",
        "                print(f\"---- new best val acc achieved {val_accuracy:.4f} ----\")\n",
        "                torch.save(model.state_dict(), chk_point_best)\n",
        "                best_eval_acc = val_accuracy\n",
        "\n",
        "            torch.save(model.state_dict(), chk_point_last)\n",
        "\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_epoch_loss:.4f}, Train Acc: {train_accuracy:.2f}, Val Loss: {total_val_loss:.4f}, Val Acc: {val_accuracy:.2f}')\n",
        "\n",
        "    return loss_train, loss_val, acc_train, acc_val\n"
      ],
      "metadata": {
        "id": "2c-hJ7PCE2T9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVlVvaWjBq1P"
      },
      "outputs": [],
      "source": [
        "def test(model,test_loader,criterion):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    prdicted_labels = [] # list of all predicted labels\n",
        "\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_data, batch_labels in test_loader:\n",
        "            batch_data = batch_data.permute(0,2,1)\n",
        "            outputs = model(batch_data.float().to(device))\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += batch_labels.size(0)\n",
        "            correct += (predicted == batch_labels.to(device)).sum().item()\n",
        "\n",
        "            val_loss += criterion(outputs, batch_labels.long().to(device)).item()\n",
        "\n",
        "            prdicted_labels = prdicted_labels + list(predicted)\n",
        "\n",
        "        val_loss /= len(test_loader)\n",
        "        accuracy = correct / total\n",
        "\n",
        "\n",
        "    print(f'Test Loss: {val_loss:.4f}, Test Acc: {accuracy:.2f}')\n",
        "\n",
        "    return val_loss , accuracy , prdicted_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers=1,bi=False):\n",
        "        super(LSTM_Encoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.bi = bi\n",
        "        # Define the LSTM layer\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True,bidirectional=bi)\n",
        "\n",
        "        if bi == True:\n",
        "            self.num_layers = self.num_layers*2\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, sequence_length, input_dim)\n",
        "        # Initialize hidden and cell state with zeros\n",
        "\n",
        "\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "\n",
        "        # Pass through the LSTM layer\n",
        "        output, (hn, cn) = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # output = output[:,-1,:]\n",
        "        # print(\"hn.shape \",hn.shape)\n",
        "        if self.bi:\n",
        "\n",
        "          hn = torch.cat((hn[0],hn[1]),dim=-1)\n",
        "        # print(\"hn.shape \",hn.shape)\n",
        "        hn = hn[-1,:,:]\n",
        "        return hn  # Return the last output\n",
        "\n",
        "\n",
        "class LSTM_Classifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes,dropout=0.7,enc_layers=2,bi=False):\n",
        "        super(LSTM_Classifier, self).__init__()\n",
        "\n",
        "        self.lstm_enc = LSTM_Encoder(input_dim,hidden_dim,enc_layers,bi)\n",
        "\n",
        "        if bi == True:\n",
        "            hidden_dim = hidden_dim*2\n",
        "\n",
        "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # self.fc2 = nn.Linear(hidden_dim, 100)\n",
        "\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.relu(self.lstm_enc(x)) # getting encodig from the lstm based encoder\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.relu(self.fc1(x))\n",
        "        # x = self.relu(self.fc2(x))\n",
        "\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "01R8G6e5tG7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y50QTH-R0-Qo"
      },
      "source": [
        "**Singature Extractor based on the Classifier**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset_train = Raw_PhysionNet(activity=\"fist_real\",sample_windows=False,include_rest=False,extract_delta=False,train=True,window_length=2,slide_delta=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaErMaipibMd",
        "outputId": "8532ab79-d49d-46ad-ba6a-c734aad7e748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== dataset configuration (mode) training ========\n",
            "\n",
            "======== include_rest -> False ========\n",
            "======== extract_delta -> False ========\n",
            "======== activity_name -> fist_real ========\n",
            "======== window_length -> 2 ========\n",
            "======== slide_delta -> 0.1 ========\n",
            "\n",
            "======== .......... ========\n",
            ".... found 327 edf files ....\n",
            "---- data from subject 88 is being excluded because of lesser sampling rate ---- \n",
            "---- data from subject 92 is being excluded because of lesser sampling rate ---- \n",
            "---- data from subject 100 is being excluded because of lesser sampling rate ---- \n",
            "---- data loaded from total of 106 -----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# raw_dataset_train_1 = Raw_PhysionNet(activity=\"fist_real\",include_rest=False,extract_delta=False,train=True,window_length=1.0,slide_delta=0.1)"
      ],
      "metadata": {
        "id": "IIa4bLanuDl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# raw_dataset_train_1_5 = Raw_PhysionNet(activity=\"fist_real\",include_rest=False,extract_delta=False,train=True,window_length=1.5,slide_delta=0.1)"
      ],
      "metadata": {
        "id": "mUwPOEgAuIND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# raw_dataset_train_2_0 = Raw_PhysionNet(activity=\"fist_real\",include_rest=False,extract_delta=False,train=True,window_length=2.0,slide_delta=0.1)"
      ],
      "metadata": {
        "id": "i_0fIrUWuK3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset_test = Raw_PhysionNet(activity=\"fist_real\",sample_windows=False,include_rest=False,extract_delta=False,train=False,window_length=1.25,slide_delta=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7hkRYcrU8dY",
        "outputId": "b231d7b6-e36c-43e8-e273-08570b270194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== dataset configuration (mode) testing ========\n",
            "\n",
            "======== include_rest -> False ========\n",
            "======== extract_delta -> False ========\n",
            "======== activity_name -> fist_real ========\n",
            "======== window_length -> 1.25 ========\n",
            "======== slide_delta -> 0.1 ========\n",
            "\n",
            "======== .......... ========\n",
            ".... found 327 edf files ....\n",
            "---- data from subject 88 is being excluded because of lesser sampling rate ---- \n",
            "---- data from subject 92 is being excluded because of lesser sampling rate ---- \n",
            "---- data from subject 100 is being excluded because of lesser sampling rate ---- \n",
            "---- data loaded from total of 106 -----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deivce = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "lIR2Zl-vxzn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTM_Classifier(input_dim=num_channels,hidden_dim=hidden_size,enc_layers=num_layers,num_classes=NUM_SUBJS,bi=False)\n",
        "criterion =nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "T9e_W60BtSf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "id": "-IIDnYBf6H1v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dd2ce36-53a4-433e-ca8c-f7e5136e4180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM_Classifier(\n",
              "  (lstm_enc): LSTM_Encoder(\n",
              "    (lstm): LSTM(64, 128, batch_first=True)\n",
              "  )\n",
              "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (dropout): Dropout(p=0.7, inplace=False)\n",
              "  (classifier): Linear(in_features=128, out_features=106, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/exp10/Copy of best_model.pth\")) # loading best model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHnpsByijc0A",
        "outputId": "a0e70154-0e58-499a-af78-7e8754b98f73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.load_state_dict(torch.load(chk_point_best))"
      ],
      "metadata": {
        "id": "cYR6AS801EdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loaders = []\n",
        "# val_loaders = []\n",
        "# for dataset in (raw_dataset_train_5,raw_dataset_train_1_5,raw_dataset_train_1,raw_dataset_train_2_0):\n",
        "#  train_loader,val_loader = gen_dataloader(dataset,[0.85,0.15])\n",
        "\n",
        "#  train_loaders.append(train_loader)\n",
        "#  val_loaders.append(val_loader)"
      ],
      "metadata": {
        "id": "iG5RyULTipuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader,val_loader = gen_dataloader(raw_dataset_train,[0.8,0.2])\n",
        "test_loader = DataLoader(raw_dataset_test,batch_size=32,shuffle=False)"
      ],
      "metadata": {
        "id": "Bm33Hf4jyO1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3JZ2Xk-Bq1Q"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JO7kLG6GzUio"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3UYVyccBq1Q"
      },
      "outputs": [],
      "source": [
        "loss_train=[]\n",
        "loss_val=[]\n",
        "acc_val= []\n",
        "acc_train = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train_loader:\n",
        "    print(i[0].shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "EKGCdIdcMWS-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "c278dc25-0b7a-4db2-ccb0-7dbf3b96fca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'tuple' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-b5f1216c9cf9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(raw_dataset_train) * 0.85"
      ],
      "metadata": {
        "id": "OdkCnzES1fzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(raw_dataset_test)"
      ],
      "metadata": {
        "id": "WuiI7W3b1vVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_eval_acc = 0\n"
      ],
      "metadata": {
        "id": "v4aPNF4Qb0Iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_variable_length(model,train_loaders,val_loaders,optimizer,criterion,1000,loss_train,loss_val,acc_train,acc_val)\n"
      ],
      "metadata": {
        "id": "JJRPmk0GikJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z56oHRjFBq1R"
      },
      "outputs": [],
      "source": [
        "train(model,train_loader,val_loader,optimizer,criterion,1000,loss_train,loss_val,acc_train,acc_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(start=0,stop=len(loss_train))\n",
        "plt.title(\"Losses\")\n",
        "plt.plot(x,loss_train,label=\"train\")\n",
        "plt.plot(x,loss_val,label=\"val\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"losses\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dp-tu-_VeLCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title(\"Accuracies\")\n",
        "plt.plot(x,acc_train,label=\"train\")\n",
        "plt.plot(x,acc_val,label=\"val\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"losses\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sH9uRuNIeqEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_on_segements():\n",
        "  losses = []\n",
        "  accs = []\n",
        "  labels = []\n",
        "\n",
        "  max = 10\n",
        "  for i in range(1,20):\n",
        "    sig_len = i/max\n",
        "\n",
        "    raw_dataset_test = Raw_PhysionNet(activity=\"fist_real\",include_rest=False,extract_delta=False,train=False,window_length=sig_len,slide_delta=0.1)\n",
        "    test_loader = DataLoader(raw_dataset_test,batch_size=32,shuffle=False)\n",
        "    y = test(model,test_loader,criterion)\n",
        "    losses.append(y[0])\n",
        "    accs.append(y[1])\n",
        "    labels.append(y[2])\n",
        "    del raw_dataset_test\n",
        "    del test_loader\n",
        "\n",
        "  return losses,accs,labels\n"
      ],
      "metadata": {
        "id": "Zk3jsoL0sh40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_incremental_segements(raw_dataset_test):\n",
        "\n",
        "  losses = []\n",
        "  accs = []\n",
        "  labels = []\n",
        "  truth = []\n",
        "\n",
        "  # raw_dataset_test = Raw_PhysionNet(activity=\"fist_real\",include_rest=False,extract_delta=False,train=False,window_length=2.0,slide_delta=0.1)\n",
        "  sample_rate = 160\n",
        "\n",
        "  x_raw = raw_dataset_test.eeg_data_x\n",
        "  x_standard = np.zeros_like(x_raw)\n",
        "\n",
        "  for idx,sample in enumerate(x_raw):\n",
        "    sample = raw_dataset_test.standardize_rows(sample)\n",
        "    x_standard[idx] = sample\n",
        "\n",
        "\n",
        "\n",
        "  max = 10\n",
        "  for i in range(1,21):\n",
        "    sig_len = i/max\n",
        "\n",
        "    x = torch.tensor(x_standard[:,:,:int(sig_len*sample_rate)])\n",
        "    print(x.shape)\n",
        "    y = torch.tensor(raw_dataset_test.eeg_data_y)\n",
        "    y = y.view(y.shape[0])\n",
        "\n",
        "    test_dataset= torch.utils.data.TensorDataset(x,y)\n",
        "\n",
        "    test_loader = DataLoader(test_dataset,batch_size=32,shuffle=False)\n",
        "    y = test(model,test_loader,criterion)\n",
        "    losses.append(y[0])\n",
        "    accs.append(y[1])\n",
        "    labels.append(y[2])\n",
        "    # truth.append(y[3])\n",
        "\n",
        "    del test_loader\n",
        "    del test_dataset\n",
        "    del x\n",
        "    del y\n",
        "\n",
        "  return losses,accs,labels,truth\n"
      ],
      "metadata": {
        "id": "B49ui_oR_ukL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses,accs,labels,truth = test_incremental_segements(raw_dataset_test)"
      ],
      "metadata": {
        "id": "eXT1oVYBBNRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(start=0.1,stop=2.1,step=0.1)"
      ],
      "metadata": {
        "id": "ORbeo5qDsYG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title(\"Signal Length Vs Time\")\n",
        "plt.plot(x,accs)\n",
        "plt.xlabel(\"time in seconds\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "GAewfhzrvPNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\"losses_test\":losses,\"accs\":accs,\"labels\":labels},os.path.join(base_dir,\"test_variable_results.pth\"))"
      ],
      "metadata": {
        "id": "90uhfFbe1fgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQcig3FMJ99r"
      },
      "outputs": [],
      "source": [
        "y = test(model,val_loaders[2],criterion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HJpuK6jwCCa_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKylky02irRj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "x = list(np.arange(1, len(loss_train)+1))\n",
        "plt.plot(np.arange(1, len(loss_val)+1),loss_val,label='loss validation')\n",
        "plt.plot(x,loss_train,label='loss train')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLySzUqfJ99r"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(1,len(acc_val)+1),acc_val,label=\"validation accuracy\")\n",
        "plt.plot(np.arange(1,len(acc_train)+1),acc_train,label=\"training accuracy\")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chk_point_best"
      ],
      "metadata": {
        "id": "2vl3qOzk4T-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3rB4nBB2-QJ"
      },
      "outputs": [],
      "source": [
        "torch.save({\"loss_val\":loss_val,\"loss_train\":loss_train,\"acc_val\":acc_val,\"acc_train\":acc_train},os.path.join(base_dir,\"last_lists.pth\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}