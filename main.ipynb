{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import PhysioNet\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### contains the definitions for the temporal encoder, spatital encoder and final model encapsulating the both ####\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math \n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 320):\n",
    "        \n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        \n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "\n",
    "        print(\"pos_embedding.shape \",pos_embedding.shape)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "        print(\"pos_embedding.shape \",pos_embedding.shape)\n",
    "        \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding):\n",
    "\n",
    "        # assuming the input is in batch_first fashion batch_size, seqlen,features\n",
    "        \n",
    "        encodings = torch.squeeze(self.pos_embedding[:token_embedding.size(1), :])\n",
    "\n",
    "\n",
    "        return self.dropout(token_embedding + encodings)\n",
    "    \n",
    "### the architecture is inspired from https://www.sciencedirect.com/science/article/pii/S1746809423005633\n",
    "\n",
    "# this is meant to extract the spatial information from the signal\n",
    "\n",
    "class Spatial_Encoder(nn.Module): \n",
    "    def __init__(self,\n",
    "                 num_heads = 8,\n",
    "                 num_layers = 2,\n",
    "                 input_features=32 # number of PSD features\n",
    "                 ):\n",
    "        \n",
    "        \n",
    "        super(Spatial_Encoder, self).__init__()\n",
    "        \n",
    "        self.enc_layer = nn.TransformerEncoderLayer(d_model=input_features, nhead=num_heads,batch_first=True)\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=self.enc_layer,num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x should be shape : batch, num_channels, num_psd_feature\n",
    "        return self.transformer_encoder(x)\n",
    "    \n",
    "\n",
    "### the architecture is inspired from https://www.nature.com/articles/s41598-022-18502-3\n",
    "class Temporal_Encoder(nn.Module): \n",
    "    def __init__(self,\n",
    "                 num_heads = 8,\n",
    "                 num_layers = 2,\n",
    "                 input_features=64 # number of channels being used\n",
    "                 ):\n",
    "        \n",
    "        \n",
    "        super(Temporal_Encoder, self).__init__()\n",
    "        \n",
    "        self.enc_layer = nn.TransformerEncoderLayer(d_model=input_features, nhead=num_heads,batch_first=True)\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=self.enc_layer,num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        # x should be shape : batch, num_channels, signal_length\n",
    "        return self.transformer_encoder(x)\n",
    "###################################################################################\n",
    "\n",
    "# used to merge the outputs of spatial and temporal encoders\n",
    "class Classification_Head(nn.Module): \n",
    "    def __init__(self,dim_temp=64,dim_spatial=32,num_classes=109):\n",
    "        \n",
    "        \n",
    "        super(Classification_Head, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=dim_temp+dim_spatial,out_features=num_classes)\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # x should be a tuple containg (tempral_outputs,spatial_outputs)\n",
    "\n",
    "        x = torch.cat(tensors=x,dim=1)\n",
    "\n",
    "        return self.fc1(x)    \n",
    "    \n",
    "##################################################################################\n",
    "\n",
    "class EEG_Transformer(nn.Module):\n",
    "    def __init__(self,num_classes = 109):\n",
    "\n",
    "        super(EEG_Transformer,self).__init__()\n",
    "\n",
    "        self.temporal_encoder = Temporal_Encoder()\n",
    "        self.spatial_encoder = Spatial_Encoder()\n",
    "        self.classification_head = Classification_Head(num_classes=num_classes)\n",
    "    \n",
    "\n",
    "    def forward(self,x):\n",
    "        assert len(x) == 2 , f\"input should be a tuple containing raw_eeg and psd_features\"\n",
    "\n",
    "        raw,psd = x\n",
    "        # print(\"raw.shape , psd.shape \",raw.shape , psd.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        info_token_psd = torch.rand(size=(psd.shape[0],1,psd.shape[-1]))\n",
    "        psd = torch.cat(tensors=(info_token_psd,psd),dim=1)\n",
    "        \n",
    "        spatial_output = self.spatial_encoder(psd) \n",
    "        # output shape is batch , seq_len, features\n",
    "\n",
    "        info_token_raw = torch.rand(size=(raw.shape[0],1,raw.shape[-1]))\n",
    "        raw = torch.cat(tensors=(info_token_raw,raw),dim=1)\n",
    "        temporal_output = self.temporal_encoder(raw)\n",
    "\n",
    "        # print(\"raw.shape , psd.shape \",raw.shape , psd.shape)\n",
    "        # print(\"temporal_output[:,0,:],spatial_output[:,0,:] \",temporal_output[:,0,:].shape,spatial_output[:,0,:].shape)\n",
    "\n",
    "\n",
    "        return self.classification_head(x=(temporal_output[:,0,:],spatial_output[:,0,:]))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_raw = torch.rand(32,320,64) # b, seq,features\n",
    "x_psd = torch.rand(32,64,32) # b, seq,features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_transformer = EEG_Transformer(num_classes=109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_embedding.shape  torch.Size([320, 64])\n",
      "pos_embedding.shape  torch.Size([320, 1, 64])\n",
      "pos_embedding.shape  torch.Size([64, 32])\n",
      "pos_embedding.shape  torch.Size([64, 1, 32])\n"
     ]
    }
   ],
   "source": [
    "temporal_encoder = PositionalEncoding(emb_size=64,dropout=0.5,maxlen=320)\n",
    "spatial_encoder = PositionalEncoding(emb_size=32,dropout=0.5,maxlen=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (63) must match the size of tensor b (64) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtemporal_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m66\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m320\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m63\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 36\u001b[0m, in \u001b[0;36mPositionalEncoding.forward\u001b[0;34m(self, token_embedding)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, token_embedding):\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# assuming the input is in batch_first fashion batch_size, seqlen,features\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding[:token_embedding\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), :])\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[43mtoken_embedding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mencodings\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (63) must match the size of tensor b (64) at non-singleton dimension 2"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 109])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_transformer((x_raw,x_psd)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== dataset configuration (mode) training ========\n",
      "\n",
      "======== k -> 3 ========\n",
      "======== sample_windows -> False ========\n",
      "======== include_rest -> False ========\n",
      "======== include_rest -> False ========\n",
      "======== extract_delta -> False ========\n",
      "======== activity_name -> fist_real ========\n",
      "======== window_length -> 0.5 ========\n",
      "======== slide_delta -> 0.1 ========\n",
      "\n",
      "======== .......... ========\n",
      ".... found 327 edf files ....\n",
      "---- data from subject 88 is being excluded because of lesser sampling rate ---- \n",
      "---- data from subject 92 is being excluded because of lesser sampling rate ---- \n",
      "---- data from subject 100 is being excluded because of lesser sampling rate ---- \n",
      "---- data loaded from total of 106 -----\n"
     ]
    }
   ],
   "source": [
    "data = PhysioNet(activity='fist_real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psds.shape  torch.Size([256, 64, 32])\n",
      "raws.shape  torch.Size([256, 80, 64])\n",
      "y.shape  torch.Size([256, 109])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(data,batch_size=256)\n",
    "for i in loader:\n",
    "    \n",
    "    \n",
    "\n",
    "    raws, psds, labels = i\n",
    "    print(\"psds.shape \",psds.shape)\n",
    "    print(\"raws.shape \",raws.shape)\n",
    "    \n",
    "    # # making the batch first false \n",
    "    # raws = raws.permute(1,0,2)\n",
    "    # psds = psds.permute(1,0,2)\n",
    "    # print(\"psds.shape \",psds.shape)\n",
    "    # print(\"raws.shape \",raws.shape)\n",
    "    \n",
    "\n",
    "\n",
    "    raws = temporal_encoder(raws)\n",
    "    psds = spatial_encoder(psds)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    y = eeg_transformer(x=(raws.float(),psds.float()))\n",
    "    print(\"y.shape \",y.shape)\n",
    "\n",
    "    # print(\"i[0].shape \",i.shape)\n",
    "    break\n",
    "\n",
    "    # input()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
